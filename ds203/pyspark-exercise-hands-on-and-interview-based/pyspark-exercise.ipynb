{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Exercise: Hands-on and Interview based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession,functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PySpark Exercise\").getOrCreate()\n",
    "# Create a DataFrame from a list of tuples\n",
    "data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 23},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 27},\n",
    "    {\"id\": 3, \"name\": \"Cathy\", \"age\": 22},\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Selecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the name and age columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 23|\n",
      "|  Bob| 27|\n",
      "|Cathy| 22|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_name_and_age = df.select(\"name\", \"age\")\n",
    "df_with_name_and_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add 5 years to the age column and create a new column named age_plus_5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------+\n",
      "|age| id| name|age_plus_5|\n",
      "+---+---+-----+----------+\n",
      "| 23|  1|Alice|        28|\n",
      "| 27|  2|  Bob|        32|\n",
      "| 22|  3|Cathy|        27|\n",
      "+---+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"age_plus_5\", df.age + 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Renaming Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename the id column to user_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "|age|user_id| name|\n",
      "+---+-------+-----+\n",
      "| 23|      1|Alice|\n",
      "| 27|      2|  Bob|\n",
      "| 22|      3|Cathy|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('id','user_id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dropping Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the user_id column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 23|Alice|\n",
      "| 27|  Bob|\n",
      "| 22|Cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('user_id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Distinct Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all distinct values in a column of your choice from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 23|\n",
      "| 27|\n",
      "| 22|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show distinct values in the age column\n",
    "df.select('age').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Basic Column Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new column that concatenates the name column with the string \"_student\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------------+\n",
      "|age| name|name_with_student|\n",
      "+---+-----+-----------------+\n",
      "| 23|Alice|    Alice_student|\n",
      "| 27|  Bob|      Bob_student|\n",
      "| 22|Cathy|    Cathy_student|\n",
      "+---+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumn('name_with_student', F.concat(F.col('name'),F.lit('_student'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Filtering Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter rows where age is greater than 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 27| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age > 25).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter rows where the name starts with the letter \"A\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 23|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.like('A%')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the total number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "count = df.count()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by age and count the number of records for each age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 23|    1|\n",
      "| 27|    1|\n",
      "| 22|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('age').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the DataFrame by name in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 23|Alice|\n",
      "| 27|  Bob|\n",
      "| 22|Cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.name.asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the DataFrame by age in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 27|  Bob|\n",
      "| 23|Alice|\n",
      "| 22|Cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.age.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Working with Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify rows with null values in any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "| age|  id| name|\n",
      "+----+----+-----+\n",
      "|  23|   1|Alice|\n",
      "|  27|NULL|  Bob|\n",
      "|NULL|   3|Cathy|\n",
      "|  20|   4|David|\n",
      "|NULL|NULL| Emma|\n",
      "+----+----+-----+\n",
      "\n",
      "+----+----+-----+\n",
      "| age|  id| name|\n",
      "+----+----+-----+\n",
      "|  27|NULL|  Bob|\n",
      "|NULL|   3|Cathy|\n",
      "|NULL|NULL| Emma|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame\n",
    "data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 23},\n",
    "    {\"name\": \"Bob\", \"age\": 27},\n",
    "    {\"id\": 3, \"name\": \"Cathy\"},\n",
    "    {\"id\": 4, \"name\": \"David\", \"age\": 20},\n",
    "    {\"name\": \"Emma\"},\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Identify if there are null values in any column\n",
    "df.filter(F.col(\"id\").isNull() | F.col(\"age\").isNull() | F.col(\"name\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace null values in the age column with the average age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|age|  id| name|\n",
      "+---+----+-----+\n",
      "| 23|   1|Alice|\n",
      "| 27|NULL|  Bob|\n",
      "| 23|   3|Cathy|\n",
      "| 20|   4|David|\n",
      "| 23|NULL| Emma|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average age\n",
    "average_age = df.agg(F.avg('age')).collect()[0][0]\n",
    "\n",
    "# Replace null values in the 'age' column with the average age\n",
    "df = df.fillna({'age': average_age})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "|  3|Cathy|\n",
      "+---+-----+\n",
      "\n",
      "+---+------------------+\n",
      "| id|initial_first_name|\n",
      "+---+------------------+\n",
      "|  1|                 A|\n",
      "|  2|                 B|\n",
      "|  4|                 D|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given two DataFrames:\n",
    "df1_data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\")]\n",
    "df1_columns = [\"id\", \"name\"]\n",
    "df1 = spark.createDataFrame(df1_data, df1_columns)\n",
    "\n",
    "df2_data = [(1, \"A\"), (2, \"B\"), (4, \"D\")]\n",
    "df2_columns = [\"id\", \"initial_first_name\"]\n",
    "df2 = spark.createDataFrame(df2_data, df2_columns)\n",
    "\n",
    "# Show them\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------------------+\n",
      "| id| name| id|initial_first_name|\n",
      "+---+-----+---+------------------+\n",
      "|  1|Alice|  1|                 A|\n",
      "|  2|  Bob|  2|                 B|\n",
      "+---+-----+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.id == df2.id,\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+------------------+\n",
      "| id| name|  id|initial_first_name|\n",
      "+---+-----+----+------------------+\n",
      "|  1|Alice|   1|                 A|\n",
      "|  2|  Bob|   2|                 B|\n",
      "|  3|Cathy|NULL|              NULL|\n",
      "+---+-----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.id == df2.id,\"left\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+------------------+\n",
      "|  id| name|  id|initial_first_name|\n",
      "+----+-----+----+------------------+\n",
      "|   1|Alice|   1|                 A|\n",
      "|   2|  Bob|   2|                 B|\n",
      "|   3|Cathy|NULL|              NULL|\n",
      "|NULL| NULL|   4|                 D|\n",
      "+----+-----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.id == df2.id,\"fullouter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left semi join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.id == df2.id,\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left anti join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  3|Cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.id == df2.id,\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Pivot Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use pivoting to calculate the average age grouped by gender and occupation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+\n",
      "|occupation|   M|   F|\n",
      "+----------+----+----+\n",
      "|     Sales|20.0|NULL|\n",
      "|   Teacher|NULL|27.0|\n",
      "|  Engineer|27.0|23.0|\n",
      "+----------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 23, \"gender\": \"F\", \"occupation\": \"Engineer\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 27, \"gender\": \"M\", \"occupation\": \"Engineer\"},\n",
    "    {\"id\": 3, \"name\": \"Cathy\", \"age\": 22, \"gender\": \"F\", \"occupation\": \"Teacher\"},\n",
    "    {\"id\": 4, \"name\": \"David\", \"age\": 20, \"gender\": \"M\", \"occupation\": \"Sales\"},\n",
    "    {\"id\": 5, \"name\": \"Emma\", \"age\": 32, \"gender\": \"F\", \"occupation\": \"Teacher\"},\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Pivot\n",
    "df.groupBy('occupation').pivot(\"gender\", [\"M\", \"F\"]).avg(\"age\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Exploding Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a column with lists, explode the list into individual rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value| id|\n",
      "+-----+---+\n",
      "|   10|  1|\n",
      "|   20|  1|\n",
      "|   30|  1|\n",
      "|   40|  2|\n",
      "|   50|  2|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [{\"id\": 1, \"values\": [10, 20, 30]}, {\"id\": 2, \"values\": [40, 50]}]\n",
    "df = spark.createDataFrame(data)\n",
    "df.select(F.explode(df.values).alias(\"value\"), df.id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Union DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine two DataFrames vertically using union. Ensure both DataFrames have the same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+---+-----+\n",
      "|age| id| name|\n",
      "+---+---+-----+\n",
      "| 23|  1|Alice|\n",
      "| 27|  2|  Bob|\n",
      "| 22|  3|Cathy|\n",
      "| 20|  4|David|\n",
      "| 32|  5| Emma|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to combine with existing df\n",
    "data1 = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 23},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 27},\n",
    "    {\"id\": 3, \"name\": \"Cathy\", \"age\": 22},\n",
    "]\n",
    "df1 = spark.createDataFrame(data1)\n",
    "data2 = [\n",
    "    {\"id\": 4, \"name\": \"David\", \"age\": 20},\n",
    "    {\"id\": 5, \"name\": \"Emma\", \"age\": 32},\n",
    "]\n",
    "df2 = spark.createDataFrame(data2)\n",
    "\n",
    "# Ensure that the schemas are the same\n",
    "df1.printSchema()\n",
    "df2.printSchema()\n",
    "\n",
    "# Union\n",
    "union_df = df1.union(df2)\n",
    "union_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview based:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is PySpark? How does it differ from Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark is the Python API for Apache Spark, a powerful tool for big data processing.  \n",
    "It allows you to analyze and process large datasets using distributed computing across multiple machines.\n",
    "\n",
    "- Scale:  \n",
    "PySpark: Handles huge datasets across clusters (big data).  \n",
    "Pandas: Limited to data that fits in your computer’s memory.\n",
    "\n",
    "- Speed:  \n",
    "PySpark: Faster for big data with in-memory distributed processing.  \n",
    "Pandas: Slower for large datasets.\n",
    "\n",
    "- Use Case:  \n",
    "PySpark: Ideal for big data and parallel processing.  \n",
    "Pandas: Best for smaller datasets and local analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What are the advantages of using PySpark over traditional Python frameworks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Handles Big Data:  \n",
    "PySpark processes massive datasets across multiple machines, unlike pandas, which is limited to single-machine memory.\n",
    "2. Faster Processing:  \n",
    "PySpark uses in-memory computing, making it much faster than traditional Python frameworks for large datasets.\n",
    "3. Scalable:  \n",
    "It works seamlessly for small to huge datasets by scaling across clusters.\n",
    "4. Supports Many Data Sources:  \n",
    "PySpark connects easily to systems like Hadoop, databases, and cloud storage.\n",
    "5. Fault-Tolerant:  \n",
    "It automatically recovers from hardware or network failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explain the concept of Resilient Distributed Dataset (RDD). How does it differ from DataFrames?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Resilient Distributed Dataset (RDD) is the basic data structure in Apache Spark. It is:\n",
    "\n",
    "- Immutable: Once created, it cannot be changed.\n",
    "- Distributed: Spread across multiple machines in a cluster.\n",
    "- Fault-Tolerant: Automatically recovers from failures.\n",
    "- Low-Level: You can control data partitioning and transformations directly.\n",
    "\n",
    "RDDs are more flexible but harder to use, while DataFrames are easier and faster for structured data tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is lazy evaluation in PySpark? Why is it useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy evaluation means PySpark doesn’t execute your operations immediately.  \n",
    "Instead, it builds a logical plan of all transformations and waits until an action (like collect(), count(), or save()) is called to execute them.  \n",
    "In short, lazy evaluation makes PySpark faster and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explain the difference between transformations and actions in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations\n",
    "- Definition: Operations that define how data is modified (e.g., map(), filter()), creating a new RDD/DataFrame without executing it.\n",
    "- Lazy: Only executed when an action is called.\n",
    "\n",
    "Actions\n",
    "- Definition: Operations that trigger computation and return results (e.g., collect(), count()).\n",
    "- Execution: Forces PySpark to process transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How is the join() operation optimized in PySpark? What are the different types of joins PySpark supports?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark optimizes joins by:\n",
    "- Broadcast Join: Uses small dataset replication across nodes for fast lookup.\n",
    "- Shuffle Join: Reorganizes data across partitions efficiently.\n",
    "\n",
    "Types of Joins in PySpark:\n",
    "- Inner Join\n",
    "- Left Outer Join\n",
    "- Right Outer Join\n",
    "- Full Outer Join\n",
    "- Cross Join\n",
    "- Semi Join\n",
    "- Anti Join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What are the key differences between PySpark DataFrames and SQL tables? Can you use SQL queries on PySpark DataFrames?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution:\n",
    "- PySpark DataFrames: Processed in a distributed manner using Spark’s engine.\n",
    "- SQL Tables: Typically stored in a database and processed using the database engine.\n",
    "\n",
    "Storage:\n",
    "- PySpark DataFrames: Can be created from various sources like HDFS, CSV, or Parquet.\n",
    "- SQL Tables: Stored in relational databases like MySQL, PostgreSQL, etc.\n",
    "\n",
    "API:\n",
    "- PySpark DataFrames: Can be manipulated using PySpark’s Python API.\n",
    "- SQL Tables: Accessed through SQL queries.\n",
    "\n",
    "You can use SQL queries on PySpark DataFrames by registering them as temporary views and querying them with spark.sql()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Explain the role of SparkSession in PySpark. Why is it needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Role of SparkSession in PySpark:\n",
    "- Entry Point: It is the main entry point for all Spark functionality.\n",
    "- Initialization: It initializes the Spark application and provides access to Spark’s APIs for reading data, creating DataFrames, and performing SQL operations.\n",
    "\n",
    "Why is it needed?\n",
    "- Simplifies API Access: Combines multiple contexts (SparkContext, SQLContext, etc.) into one unified interface.\n",
    "- Manages Resources: Handles the configuration of Spark applications and resource management.\n",
    "\n",
    "In short, SparkSession is essential for interacting with Spark in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What is the difference between SparkContext and SparkSession? When to use each of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext:\n",
    "- Low-level entry point for Spark functionality, primarily used for working with RDDs and managing the cluster.\n",
    "- Essential for handling distributed data processing.\n",
    "\n",
    "SparkSession:\n",
    "- Higher-level entry point, introduced in Spark 2.0, that includes SparkContext and provides additional functionality for DataFrames, SQL, and other Spark features.\n",
    "- Simplifies accessing Spark's various APIs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
